{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/wandb/sdk/launch/builder/build.py:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/ml4ig1/miniconda3/envs/training-gpt/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from typing import Union\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from scgpt import logger\n",
    "\n",
    "from scgpt.trainer import train as scgpt_train\n",
    "from scgpt.trainer import evaluate as scgpt_evaluate\n",
    "from scgpt.trainer import eval_testdata as scgpt_test\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.utils import eval_scib_metrics, load_pretrained\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "from scgpt_helper import *\n",
    "\n",
    "import lamindb as ln\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment = scGExperiment()\n",
    "batch_keys = [\n",
    "    \"self_reported_ethnicity_ontology_term_id\",\n",
    "    \"assay_ontology_term_id\",\n",
    "]\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<mask>\"]\n",
    "n_hvg=2000\n",
    "filter_gene_by_counts = 3\n",
    "data_is_raw = True\n",
    "save_path = \"../save/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"../save/scGPT_human/vocab.json\"\n",
    "model_path = \"../save/scGPT_human/best_model.pt\"\n",
    "batch_size = 8\n",
    "epoch = 5\n",
    "fast_transformer=True\n",
    "mask_ratio=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../save/scGPT_human/args.json\", 'r') as f:\n",
    "    config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ln.File.filter(uid=ln.File.search(\"retina\").index[0]).one().load()\n",
    "# adata = ln.File.filter().first().load()\n",
    "#ln.Dataset.using(\"laminlabs/cellxgene-census\").one()\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 19694 × 37127\n",
       "    obs: 'n_genes', 'n_counts', 'percent_mito', 'donor_id', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'is_primary_data', 'organism_ontology_term_id', 'sex_ontology_term_id', 'tissue_ontology_term_id', 'author_cell_type', 'suspension_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage'\n",
       "    var: 'chromosome', 'featureend', 'featurestart', 'n_cells', 'percent_cells', 'robust', 'highly_variable_features', 'mean', 'var', 'hvf_loess', 'hvf_rank', 'gene_symbols', 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype'\n",
       "    uns: 'cell_type_ontology_term_id_colors', 'default_embedding', 'schema_version', 'title'\n",
       "    obsm: 'X_diffmap', 'X_diffmap_pca', 'X_fitsne', 'X_fle', 'X_pca', 'X_phi', 'X_umap'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 19694 × 37127\n",
       "    obs: 'n_genes', 'n_counts', 'percent_mito', 'donor_id', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'is_primary_data', 'organism_ontology_term_id', 'sex_ontology_term_id', 'tissue_ontology_term_id', 'author_cell_type', 'suspension_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage'\n",
       "    var: 'chromosome', 'featureend', 'featurestart', 'n_cells', 'percent_cells', 'robust', 'highly_variable_features', 'mean', 'var', 'hvf_loess', 'hvf_rank', 'gene_symbols', 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'id_in_vocab'\n",
       "    uns: 'cell_type_ontology_term_id_colors', 'default_embedding', 'schema_version', 'title'\n",
       "    obsm: 'X_diffmap', 'X_diffmap_pca', 'X_fitsne', 'X_fle', 'X_pca', 'X_phi', 'X_umap'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = setup(dataset_name, save_path, config)\n",
    "\n",
    "if type(vocab) is str:\n",
    "    vocab = GeneVocab.from_file(vocab)\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "dataset = load_dataset(adata, vocab)\n",
    "dataset.obs[\"batch_id\"] = dataset.obs[batch_keys].apply(\"_\".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.init_datamodule(dataset=, vocab=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - WARNING - No batch_key is provided, will use all cells for HVG selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config['n_bins'],  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ml4ig1/Documents code/scGPT/mytests/cleanup_run.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data_loader, valid_loader \u001b[39m=\u001b[39m prepare_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     vocab,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     n_hvg\u001b[39m=\u001b[39;49mn_hvg,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     mask_ratio\u001b[39m=\u001b[39;49mmask_ratio\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bperso/home/ml4ig1/Documents%20code/scGPT/mytests/cleanup_run.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents code/scGPT/mytests/scgpt_helper.py:54\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(dataset, vocab, batch_size, epoch, mask_ratio, mask_value, n_hvg, pad_token, pad_value, per_seq_batch_sample, test_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m all_counts \u001b[39m=\u001b[39m (\n\u001b[1;32m     36\u001b[0m     dataset\u001b[39m.\u001b[39mlayers[\u001b[39m\"\u001b[39m\u001b[39mX_binned\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m issparse(dataset\u001b[39m.\u001b[39mlayers[\u001b[39m\"\u001b[39m\u001b[39mX_binned\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m dataset\u001b[39m.\u001b[39mlayers[\u001b[39m\"\u001b[39m\u001b[39mX_binned\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m (\n\u001b[1;32m     41\u001b[0m     train_data,\n\u001b[1;32m     42\u001b[0m     valid_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m tokenized_train \u001b[39m=\u001b[39m tokenize_and_pad_batch(\n\u001b[1;32m     55\u001b[0m     train_data,\n\u001b[1;32m     56\u001b[0m     dataset\u001b[39m.\u001b[39;49mvar[\u001b[39m\"\u001b[39;49m\u001b[39mgene_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     57\u001b[0m     max_len\u001b[39m=\u001b[39;49mn_hvg \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     58\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     59\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m     60\u001b[0m     pad_value\u001b[39m=\u001b[39;49mpad_value,\n\u001b[1;32m     61\u001b[0m     append_cls\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# append <cls> token at the beginning\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m     include_zero_gene\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m tokenized_valid \u001b[39m=\u001b[39m tokenize_and_pad_batch(\n\u001b[1;32m     65\u001b[0m     valid_data,\n\u001b[1;32m     66\u001b[0m     dataset\u001b[39m.\u001b[39mvar[\u001b[39m\"\u001b[39m\u001b[39mgene_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     include_zero_gene\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     75\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain set number of samples: \u001b[39m\u001b[39m{\u001b[39;00mtokenized_train[\u001b[39m'\u001b[39m\u001b[39mgenes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m feature length: \u001b[39m\u001b[39m{\u001b[39;00mtokenized_train[\u001b[39m'\u001b[39m\u001b[39mgenes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m )\n",
      "File \u001b[0;32m~/Documents code/scGPT/scgpt/tokenizer/gene_tokenizer.py:419\u001b[0m, in \u001b[0;36mtokenize_and_pad_batch\u001b[0;34m(data, gene_ids, max_len, vocab, pad_token, pad_value, append_cls, include_zero_gene, cls_token, return_pt, mod_type, vocab_mod)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_and_pad_batch\u001b[39m(\n\u001b[1;32m    403\u001b[0m     data: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    404\u001b[0m     gene_ids: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     vocab_mod: Vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    Tokenize and pad a batch of data. Returns a list of tuple (gene_id, count).\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     cls_id \u001b[39m=\u001b[39m vocab[cls_token]\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m mod_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m         cls_id_mod_type \u001b[39m=\u001b[39m vocab_mod[cls_token]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "data_loader, valid_loader = prepare_dataset(\n",
    "    dataset,\n",
    "    vocab,\n",
    "    batch_size,\n",
    "    epoch=epoch,\n",
    "    n_hvg=n_hvg,\n",
    "    test_size=0.2,\n",
    "    mask_ratio=mask_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    len(vocab),  # n_tokens\n",
    "    # TODO:\n",
    "    config['embsize'],\n",
    "    config['nhead'],\n",
    "    config['d_hid'],\n",
    "    config['nlayers'],\n",
    "    vocab=vocab,\n",
    "    dropout=config['dropout'],\n",
    "    pad_token=config['pad_token'],\n",
    "    pad_value=config['pad_value'],\n",
    "    do_mvc=,\n",
    "    do_dab=True,\n",
    "    use_batch_labels=True,\n",
    "    num_batch_labels=len(set(dataset.obs[\"batch_id\"])),\n",
    "    domain_spec_batchnorm=config['DSBN'],\n",
    "    n_input_bins=config['n_input_bins'],\n",
    "    ecs_threshold=config['ecs_thres'],\n",
    "    explicit_zero_prob=config['explicit_zero_prob'],\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    pre_norm=config['pre_norm'],\n",
    ")\n",
    "if model_path is not None:\n",
    "    load_pretrained(model, torch.load(model_path), verbose=False)\n",
    "    # model_config['file'] = model_dir / \"args.json\"\n",
    "    # model_file = model_dir / \"best_model.pt\"\n",
    "model.to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: init wandb\n",
    "criterion = masked_mse_loss\n",
    "criterion_dab = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    eps=1e-4 if config['amp'] else 1e-8,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, 1, gamma=config['schedule_ratio']\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config['amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tune and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    config=config,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"vocab\": vocab,\n",
    "        \"config\": config,\n",
    "    },\n",
    "    save_dir+\"/best_model.pt\",\n",
    ")\n",
    "\n",
    "wandb.use_artifact(\n",
    "    save_dir + \"/best_model.pt\", type=\"model\"\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scgpt_test(\n",
    "    model, adata_test, gene_ids, vocab, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
